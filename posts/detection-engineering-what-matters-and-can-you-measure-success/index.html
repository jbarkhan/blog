<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-gb dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>detection engineering: what matters and can you measure success? - null thought</title>
<meta name=theme-color><meta name=description content="a beginning
I have been thinking lately about detection engineering. Specifically, I have been thinking about these two questions as they relate to a detection engineering program:

What matters?
Can you measure success?

I am not sure I have a good answer to either of these questions yet and there isn&rsquo;t a lot of authoritative information out there. As far as I can tell, detection engineering is a somewhat specialised and relatively new field within cyber security. It seems like most organisations are doing their own thing and only sufficiently mature organisations appear to have dedicated detection engineering teams."><meta name=author content="notes"><link rel="preload stylesheet" as=style href=https://nullthought.dev/main.min.css><link rel=preload as=image href=https://nullthought.dev/theme.svg><link rel=preload as=image href=https://nullthought.dev/github.svg><link rel=preload as=image href=https://nullthought.dev/linkedin.svg><script defer src=https://nullthought.dev/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://nullthought.dev/favicon.ico><link rel=apple-touch-icon href=https://nullthought.dev/apple-touch-icon.png><meta name=generator content="Hugo 0.147.3"><meta itemprop=name content="detection engineering: what matters and can you measure success?"><meta itemprop=description content="a beginning I have been thinking lately about detection engineering. Specifically, I have been thinking about these two questions as they relate to a detection engineering program:
What matters? Can you measure success? I am not sure I have a good answer to either of these questions yet and there isn’t a lot of authoritative information out there. As far as I can tell, detection engineering is a somewhat specialised and relatively new field within cyber security. It seems like most organisations are doing their own thing and only sufficiently mature organisations appear to have dedicated detection engineering teams."><meta itemprop=datePublished content="2025-05-18T00:00:00+00:00"><meta itemprop=dateModified content="2025-05-18T00:00:00+00:00"><meta itemprop=wordCount content="2812"><meta property="og:url" content="https://nullthought.dev/posts/detection-engineering-what-matters-and-can-you-measure-success/"><meta property="og:site_name" content="null thought"><meta property="og:title" content="detection engineering: what matters and can you measure success?"><meta property="og:description" content="a beginning I have been thinking lately about detection engineering. Specifically, I have been thinking about these two questions as they relate to a detection engineering program:
What matters? Can you measure success? I am not sure I have a good answer to either of these questions yet and there isn’t a lot of authoritative information out there. As far as I can tell, detection engineering is a somewhat specialised and relatively new field within cyber security. It seems like most organisations are doing their own thing and only sufficiently mature organisations appear to have dedicated detection engineering teams."><meta property="og:locale" content="en_gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="detection engineering: what matters and can you measure success?"><meta name=twitter:description content="a beginning I have been thinking lately about detection engineering. Specifically, I have been thinking about these two questions as they relate to a detection engineering program:
What matters? Can you measure success? I am not sure I have a good answer to either of these questions yet and there isn’t a lot of authoritative information out there. As far as I can tell, detection engineering is a somewhat specialised and relatively new field within cyber security. It seems like most organisations are doing their own thing and only sufficiently mature organisations appear to have dedicated detection engineering teams."><link rel=canonical href=https://nullthought.dev/posts/detection-engineering-what-matters-and-can-you-measure-success/></head><body class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"><div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto"><a class="-translate-y-[1px] text-2xl font-medium" href=https://nullthought.dev/>null thought</a><div class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/>home</a><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/whoami/>whoami</a></nav><nav class="mt-12 flex justify-center space-x-10 lg:mt-0 lg:items-center ltr:lg:ml-14 rtl:space-x-reverse rtl:lg:mr-14 dark:invert"><a class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/jbarkhan target=_blank rel=me>github</a>
<a class="h-7 w-7 text-[0px] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./linkedin.svg) href=https://linkedin.com/in/jeromebarkhan target=_blank rel=me>linkedin</a></nav></div></header><main class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"><article><header class=mb-14><h1 class="my-0! pb-2.5">detection engineering: what matters and can you measure success?</h1><div class="text-xs antialiased opacity-60"><time>18 May 2025</time>
· 14 minute read</div></header><section><h1 id=a-beginning>a beginning</h1><p>I have been thinking lately about detection engineering. Specifically, I have been thinking about these two questions as they relate to a detection engineering program:</p><ol><li>What matters?</li><li>Can you measure success?</li></ol><p>I am not sure I have a good answer to either of these questions yet and there isn&rsquo;t a lot of authoritative information out there. As far as I can tell, detection engineering is a somewhat specialised and relatively new field within cyber security. It seems like most organisations are doing their own thing and only sufficiently mature organisations appear to have dedicated detection engineering teams.</p><p>For context, when I talk about detection engineering, I am referring to the apparent evolution that has taken place from the primitive concept of security monitoring. The fundamentals are the same, but the former builds on the latter by incorporating engineering methods and principles such as lifecycle modelling and optimisation as well as DevOps practices like something-as-code and automated testing.</p><p>Despite the limited amount of information, some of the concepts I have come across in my research might be constituent parts of good answers and are helpful in their current form. I aim to explore some of those in this post and future posts.</p><p>There are several reasons I have been thinking about detection engineering. Firstly, I work in detection and response and it constitute a large part of what I do day-to-day. Secondly, and more importantly, I have come to appreciate the significant way in which it underpins and enables defensive operational awareness and the role it plays in causing the adversary a great deal of pain.</p><p>My plan is to start with discussing these questions at a high level and work my way down to the details. But I make no promises to adhere to that plan. To that end, I have one outcome I am ultimately trying to achieve on this journey:</p><blockquote><p>Fix a bit of the murky thought cloud in my head by distilling it into writing that makes a little bit of sense.</p></blockquote><p>If a by-product of this is developing guidance that is simple, non-prescriptive, and easy to apply in practice for thinking about, maturing, and managing a detection engineering program, then I will have exceeded my own expectations.</p><p>As I progress, it is likely that I will discover new information and change how I see things. I will revisit and update or make new posts when that happens. I will also probably be making a lot of implicit assumptions about what you know, this is selfishly written for me.</p><h1 id=what-matters>what matters?</h1><p>I first want to start with purpose. Without purpose, it is difficult to be coherent and make sense of why and how we do things. Ignoring the potential philosophical implications of that statement, I take the liberty of assuming it is true. Moving forward, I will use the following definition of purpose for detection engineering.</p><dl><dt>Purpose</dt><dd>To continuously convert threat knowledge into signals that reveal adversary behaviour in your environment and enable the timely disruption of their progress toward malicious goals.</dd></dl><p>This definition is broader than most I have come across. I like it for two main reasons:</p><ol><li>It highlights natural and important connections with three other key functions in the wider defensive program; <strong>threat intelligence</strong>, <strong>threat hunting</strong>, and <strong>incident response</strong>.</li><li>It alludes to <em>signal detection theory</em>, a well established field of study with methods and results that we can use to better understand the problem space and optimise.</li></ol><p><img src=/images/connection.svg alt=Connection></p><p>To expand on the first reason briefly:</p><ul><li>Understanding your environment&rsquo;s threat profile is crucial for focusing your attention on surfacing signals that are most relevant. You have a limited amount of resources and they should be used accordingly. If your most salient threat is a ransomware gang, you likely need to focus on a slightly different set of signals compared to a nation state. They go about their business in different ways and this will be evident in the data.</li><li>The process of converting knowledge into signals in this context sounds suspiciously like a threat hunt. In my view, back testing a detection rule during development is threat hunting. It may not be particularly sophisticated as far as threat hunting goes, but if your rule is threat informed then you are nonetheless doing a threat hunt.</li><li>Disrupting the adversary before they can achieve malicious goals is the ideal scenario for incident response. If you detect the adversary after they have achieved their goals, your detection has failed and your organisation and responders are probably not having a good time.</li></ul><p>On the second reason, a detection rule is nothing more than a type of <em>binary classifier</em>. For simplicity, and without much loss of generality, if we restrict our definition of detection rule to those that are search based, a detection rule $r$ can be viewed conceptually as a composition of two functions:</p><ul><li>A signal extraction function $s$ that maps data $x\in X$ to signals.</li><li>A decision function $d$ that maps those signals to a binary threat classification.</li></ul><p>That is to say, the rule $r$ computes a threat label $y\in (\text{threat}, \text{no threat})$ as:
$$
y = r(x) = d(s(x))
$$</p><p>This abstraction is helpful for highlighting how separate features of a rule can be evaluated and optimised independently. We can improve fidelity through $s$ and tune the decision boundary through $d$. For example, $s$ may catch encoded PowerShell commands and $d$ includes logic to determine which are malicious.</p><p>There are existing tools and methods that we can borrow from signal detection theory for measuring the performance of binary classifiers, understanding the nature of tuning them, and recognising their limitations. Taking advantage of these tools and methods can also provide insights that will allow us to avoid mistakes that lead to common issues in the detection and response space. For brevity, I will refrain from listing them. If you have spent time triaging alerts then you know.</p><p>Given this purpose, I think that there are ultimately three variables that matter; coverage, quality, and capacity. Anything that you do as part of a detection engineering program should contribute in some way to improving one or more of them. I suggest further that the objective of a detection engineering strategy should be to maximise a function $f$ of these variables subject to some constraints, for example budget or head count. It is a constrained optimisation problem.</p><div>$$
\begin{aligned}
\text{max:} \quad & f(\text{coverage}, \text{quality}, \text{capacity}) \\
\text{subject to:} \quad & \text{constraints}
\end{aligned}
$$</div><p><img src=/images/strategy.svg alt=Strategy></p><h2 id=1-coverage>1. Coverage</h2><p>Coverage refers to the extent to which your detection rules collectively target surfacing the range of signals that are known to be created by threat actors. A <em>coverage gap</em> presents the adversary with an opportunity to achieve their goals without detection. There are many ways to think about what this means. The contemporary example is facilitated by MITRE ATT&amp;CK through tactics and techniques. You can build a map between your rules that surface tactics and techniques and the ATT&amp;CK library to visualise or measure your coverage.</p><p>Given limited resources, you can constrain the library by prioritising tactics and techniques based on your threat profile. <a href=https://detect.fyi/detection-engineering-lifecycle-an-integrated-approach-to-threat-detection-and-response-54de5bf17dba>This post</a> outlines a prioritisation method along these lines. Of course, tactics and techniques are not the only signal we aim to surface, but they do sit at the top of the <a href=https://www.attackiq.com/glossary/pyramid-of-pain/>Pyramid of Pain</a>.</p><p><img src=/images/coverage.svg alt=Coverage></p><p>Since your rules are dependent on data from your environment, coverage also entails in some way that you have the data that is required for your rule set. <strong>You cannot detect what you cannot see</strong> and seeing is having the data available for your rules to function.</p><p>It is also important to acknowledge the fact that the adversary is not stationary. They adapt. The threat landscape is constantly evolving. Your rule set should be equally adaptable. If your rule set lags behind changes to your threat profile then you end up with a coverage gap. Continual assessment of coverage is necessary.</p><h2 id=2-quality>2. Quality</h2><p>Quality reflects mainly how well your rule is documented, promotes triage efficiency through context, and it&rsquo;s performance. However, I think quality is a complex variable with more contributing factors. To expand on just the three highlighted:</p><ol><li><p><strong>Documentation</strong>: we want the rule to be accompanied by documentation that at least facilitates easily understanding:</p><ul><li>The intention, goal, or objective of the rule - what signal is it trying to surface?</li><li>Why the signal matters - is this worth the time that your incident response team will spend investigating?</li><li>How the rule works - what data is required and how is signal extracted?</li><li>Expected triage or response actions - how do you arrive at a verdict and how should you respond?</li><li>Steps to test or trigger the rule - how can you verify that it works?</li></ul><p>Palantir have a blog post outlining their <a href=https://blog.palantir.com/alerting-and-detection-strategy-framework-52dc33722df2>Alerting and Detection Strategy Framework</a> that covers this ground in more detail.</p></li><li><p><strong>Context</strong>: we want the rule to provide sufficient information about the triggering signal in an alert so that triage time can be minimised by:</p><ul><li>Avoiding excessive manual investigative effort required to reach a verdict.</li><li>Automating the required response where possible.</li></ul></li><li><p><strong>Performance</strong>: we want the rule to surface signals that constitute real threats and not noise. There are several performance metrics associated with binary classifiers we can lean on here. I have to stress that not all metrics are equal and it is crucial to understand their strengths and weaknesses. This is especially true when signals are rare and the base rate fallacy is at play. The most accurate rule can have low precision and a high false positive rate.</p></li></ol><h2 id=3-capacity>3. Capacity</h2><p>At the end of the day, your triage team can only do so much. All else being equal, there will be a certain alert rate that your team is capable of handling before they start to experience burnout or make investigative compromises and mistakes. This rate is what I call <em>capacity</em>. The alert rate must be carefully matched to capacity.</p><p>Jai Minton has a <a href=https://www.jaiminton.com/internal-blog/high-impact-security-analysis#>blog post</a> that, among other things, walks through the triage process and explains in detail the analyst mindset. In particular, he relates the concept of the iron triangle to alert triage. He says that security analysis can be good, fast, or cheap, but you can only ever pick at most 2 in any given instance. Given the sobering reality of a budget and applying this concept to a triage team collectively, I suggest further that there is actually an upper bound on speed and it can easily be overwhelmed by:</p><ul><li>A poor quality rule set.</li><li>A fragmented detection surface with tools that are insufficiently scrutinised or improperly configured.</li></ul><p>When faced with this situation you can either retain your good security analysis, amass alert debt, and blow out all triage and response time metrics, or forgo good security analysis.</p><p><img src=/images/fatigue.svg alt=Fatigue></p><p>Like quality, capacity is somewhat complex and there are many contributing factors. These include:</p><ul><li>The number of analysts you have.</li><li>The experience, expertise, and motivation of your analysts.</li><li>The maturity of your triage processes.</li><li>The analysis tools available.</li><li>How much of your alert triage and response is automated.</li></ul><p>How many of these factors the detection engineering program shares responsibility for will depend on the broader cyber security operating model. But it should be acutely aware of all of them.</p><h1 id=can-you-measure-success>can you measure success?</h1><p>Many parts of theory, science, and professions are concerned with measuring things. There are good reasons for this. Measurement allows us to test assumptions, understand the effects of change, make informed decisions, and build bridges that don&rsquo;t collapse. If you can measure something, it is easier to manage and optimise. It is also often hard to do, especially when uncertainty plays a large role. Risk, for example, is defined by uncertainty, and entire professions are dedicated to understanding and quantifying it.</p><p>There are good reasons to measure the success of a detection engineering program. But what does success even mean in this context? I think this is actually quite a hard question to answer. If we base success on the established purpose, then success implies at least identifying the presence of all adversaries before they can achieve their objectives.</p><p>The problem is that you don&rsquo;t know the ground truth. You can&rsquo;t know a priori if an adversary is present or not and you can&rsquo;t easily prove their absence. If your detection rules don&rsquo;t surface adversary behaviour it doesn&rsquo;t necessarily mean one isn&rsquo;t there. Absence of evidence is not necessarily evidence of absence. You could just have bad detection rules. And that&rsquo;s fine, because think of all the room for improvement.</p><p>On the other hand, it can be very easy to tell if you have failed (assuming sufficiently competent incident response):</p><ul><li>If your data ends up for sale on the dark web, you have probably failed.</li><li>If your business operations are disrupted by ransomware, you have probably failed.</li><li>If you end up in the news for a breach, you have probably failed.</li><li>If your incident response team discovers attacker activity that should have triggered a detection rule but didn&rsquo;t, you have failed.</li></ul><p>What these examples serve to highlight is an asymmetry. While failure tends to be visible and costly, success is quieter, harder to grasp, and may go unrecognised. This asymmetry and the intangible nature of success poses a challenge. I suspect that the best we can do in practice to measure success is to use a tailored combination of:</p><ol><li>Experience based heuristics.</li><li>Continuous testing.</li><li>Proxy metrics.</li></ol><h2 id=1-experience-based-heuristics>1. Experience Based Heuristics</h2><p>Under this category I place all of the maturity models and adjacent frameworks. These seem to be largely constructed from aggregated experience. They cannot tell you directly if you are succeeding. But they can provide guidance that allows you to situate and orient your program so that it is moving in the right direction (at least according to the collective experience of seasoned practitioners or experts).</p><p>Some notable references in this space include:</p><ul><li>Elastic&rsquo;s <a href=https://www.elastic.co/security-labs/elastic-releases-debmm>Detection Engineering Behavioural Maturity Model</a>. I am partial to Elastic because they understand that trying to do detection engineering without a normalised schema is like trying to find your asset inventory in the Library of Babel with a magnet and a pair of dowsing rods.</li><li>Ryan Stillion&rsquo;s <a href=https://ryanstillions.blogspot.com/2014/04/the-dml-model_21.html>Detection Maturity Level model</a>. A great read that pre-dates the release of ATT&amp;CK for Enterprise and covers a lot of ground on, and adjacent to, tactics and techniques.</li><li>Kyle Bailey&rsquo;s <a href=https://detectionengineering.io/>Detection Engineering Maturity Matrix</a>. It is comprehensive at a high level but concise enough to fit on a single page.</li></ul><h2 id=2-continuous-testing>2. Continuous Testing</h2><p>To know if something is working you should test it. If something changes, you should test it again. If some time has passed since you last tested it, you should also test it again. In this case there are two primary methods that I think apply:</p><ol><li>Unit testing.</li><li>Adversary emulation.</li></ol><h3 id=unit-testing>Unit Testing</h3><p>For each rule in your rule set you should know how to generate signals that the rule will surface. You should create these signals on a regular basis in a clean way in your environment to trigger the rule and confirm that the alert triggered correctly. Ideally this process is automated and bypasses triage.</p><h3 id=adversary-emulation>Adversary Emulation</h3><p>One team, two team, red team, blue team, or purple team. Whatever you want to call it, pretend to be the adversary and do the thing. The more the better. If you can&rsquo;t do it, get someone else to do it. Did you detect all the things? Great! Otherwise, you have work to do.</p><p>There is a recent trend among ransomware groups where they will provide their victims with a detailed report outlining how they gained access after payment. This sounds cool, almost like a legitimate business service. But you can get the same outcome with more certainty, less pain, and for fewer gold coins from people who are not financially motivated criminals. Plus, there are all the other kinds of adversaries that have no incentive to tell you how they did the thing.</p><h2 id=3-proxy-metrics>3. Proxy Metrics</h2><p>These are statistics you can calculate, sometimes easily, but that are not directly measuring detection engineering success. Rather, you might be able to infer something about success from them. Examples include:</p><ul><li>Rule set coverage indicator.</li><li>Mean time to detect.</li><li>Mean time to triage.</li><li>Alert fatigue indicator.</li></ul><p>None of these are going to be particularly informative and they are almost always confounded by factors that you might not be aware of or that are outside your remit. Be careful when calculating and interpreting these statistics.</p><h1 id=to-summarise>to summarise</h1><p>I think there are three variables that matter:</p><ul><li>Coverage</li><li>Quality</li><li>Capacity</li></ul><p>A detection engineering program should aim to maximise a function of these variables subject to organisational constraints.</p><p>Defining and measuring success is hard because success is somewhat intangible and there is an asymmetry between success and failure. It is likely the best we can do for now is to use a combination of experience based heuristics, continuous testing, and proxy metrics.</p></section></article></main><footer class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"><div class=mr-auto>&copy;2025
<a class=link href=https://nullthought.dev/>null thought</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a>
<script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "21e6dd06c7e649858b7ba0bcdeff84c7"}'></script></footer></body></html>